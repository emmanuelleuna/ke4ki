{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a66a653",
   "metadata": {
    "_cell_guid": "6dfa333f-39b6-4a34-ab77-5cbf3afa27f4",
    "_uuid": "87a629f1-5a65-484a-9c83-c40109d634c3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007594,
     "end_time": "2025-10-09T13:03:41.015943",
     "exception": false,
     "start_time": "2025-10-09T13:03:41.008349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clear Kaggle working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56653852",
   "metadata": {
    "_cell_guid": "6a10139a-3c29-46fb-a285-65b96f2f70d2",
    "_uuid": "f3988a89-920a-48c4-a7e5-92781a85abd0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-09T13:03:41.029313Z",
     "iopub.status.busy": "2025-10-09T13:03:41.029119Z",
     "iopub.status.idle": "2025-10-09T13:03:41.032883Z",
     "shell.execute_reply": "2025-10-09T13:03:41.032216Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011548,
     "end_time": "2025-10-09T13:03:41.033900",
     "exception": false,
     "start_time": "2025-10-09T13:03:41.022352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Répertoire de travail Kaggle\n",
    "# working_dir = \"/kaggle/working\"\n",
    "\n",
    "# # Parcours tous les fichiers et dossiers dans /kaggle/working\n",
    "# for filename in os.listdir(working_dir):\n",
    "#     file_path = os.path.join(working_dir, filename)\n",
    "#     try:\n",
    "#         # Supprime fichier ou dossier\n",
    "#         if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "#             os.unlink(file_path)  # Supprimer fichier ou lien symbolique\n",
    "#         elif os.path.isdir(file_path):\n",
    "#             shutil.rmtree(file_path)  # Supprimer dossier récursivement\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erreur lors de la suppression de {file_path}: {e}\")\n",
    "\n",
    "# print(\"Kaggle working cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a040ab9",
   "metadata": {
    "_cell_guid": "ffdebdd3-6c3f-41cc-9a73-a74d2c9e9724",
    "_uuid": "6634abbc-7026-43fb-b73f-29087848348a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005979,
     "end_time": "2025-10-09T13:03:41.046121",
     "exception": false,
     "start_time": "2025-10-09T13:03:41.040142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install Necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c11593f",
   "metadata": {
    "_cell_guid": "328d2dee-476d-4f89-b852-d2ea46b9ca64",
    "_uuid": "d775da29-d65d-4dc3-b7d2-348d2be408a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-09T13:03:41.060065Z",
     "iopub.status.busy": "2025-10-09T13:03:41.059448Z",
     "iopub.status.idle": "2025-10-09T13:04:05.351159Z",
     "shell.execute_reply": "2025-10-09T13:04:05.350206Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 24.299573,
     "end_time": "2025-10-09T13:04:05.352472",
     "exception": false,
     "start_time": "2025-10-09T13:03:41.052899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\r\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\r\n",
      "Collecting rouge_score\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting json-repair\r\n",
      "  Downloading json_repair-0.52.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\r\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\r\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\r\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading json_repair-0.52.0-py3-none-any.whl (26 kB)\r\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\r\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0bf25206a5e561de03f61a69ba9b06128ed0b1a7818be3e978175edd5daae55e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\r\n",
      "Successfully built rouge_score\r\n",
      "Installing collected packages: json-repair, fsspec, rouge_score, evaluate\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.5.1\r\n",
      "    Uninstalling fsspec-2025.5.1:\r\n",
      "      Successfully uninstalled fsspec-2025.5.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\r\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\r\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.6 fsspec-2025.3.0 json-repair-0.52.0 rouge_score-0.1.2\r\n",
      "Collecting orkg\r\n",
      "  Downloading orkg-1.0.1-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: requests_toolbelt in /usr/local/lib/python3.11/dist-packages (1.0.0)\r\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (2.5.0)\r\n",
      "Collecting pdfplumber\r\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: Deprecated<2.0.0,>=1.2.14 in /usr/local/lib/python3.11/dist-packages (from orkg) (1.2.18)\r\n",
      "Collecting Faker<20.0.0,>=19.1.0 (from orkg)\r\n",
      "  Downloading Faker-19.13.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting Inflector<4.0.0,>=3.1.0 (from orkg)\r\n",
      "  Downloading Inflector-3.1.1-py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Collecting cardinality<0.2.0,>=0.1.1 (from orkg)\r\n",
      "  Downloading cardinality-0.1.1.tar.gz (2.3 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting hammock<0.3.0,>=0.2.4 (from orkg)\r\n",
      "  Downloading hammock-0.2.4.tar.gz (4.8 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting loguru<0.8.0,>=0.7.2 (from orkg)\r\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\r\n",
      "Requirement already satisfied: networkx<4.0,>=3.1 in /usr/local/lib/python3.11/dist-packages (from orkg) (3.5)\r\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from orkg) (2.2.3)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from orkg) (2.11.7)\r\n",
      "Collecting python-keycloak<6.0.0,>=5.7.0 (from orkg)\r\n",
      "  Downloading python_keycloak-5.8.1-py3-none-any.whl.metadata (6.0 kB)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from orkg) (2.32.4)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from orkg) (4.67.1)\r\n",
      "Collecting undecorated<0.4.0,>=0.3.0 (from orkg)\r\n",
      "  Downloading undecorated-0.3.0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\r\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\r\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\r\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\r\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (44.0.3)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated<2.0.0,>=1.2.14->orkg) (1.17.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from Faker<20.0.0,>=19.1.0->orkg) (2.9.0.post0)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.0.1->orkg) (1.26.4)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.0.1->orkg) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.0.1->orkg) (2025.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->orkg) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->orkg) (2.33.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->orkg) (4.14.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->orkg) (0.4.1)\r\n",
      "Collecting aiofiles>=24.1.0 (from python-keycloak<6.0.0,>=5.7.0->orkg)\r\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting async-property>=0.2.2 (from python-keycloak<6.0.0,>=5.7.0->orkg)\r\n",
      "  Downloading async_property-0.2.2-py2.py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Collecting deprecation>=2.1.0 (from python-keycloak<6.0.0,>=5.7.0->orkg)\r\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Requirement already satisfied: httpx>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from python-keycloak<6.0.0,>=5.7.0->orkg) (0.28.1)\r\n",
      "Collecting jwcrypto>=1.5.4 (from python-keycloak<6.0.0,>=5.7.0->orkg)\r\n",
      "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->orkg) (3.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->orkg) (2025.6.15)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation>=2.1.0->python-keycloak<6.0.0,>=5.7.0->orkg) (25.0)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.2->python-keycloak<6.0.0,>=5.7.0->orkg) (4.9.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.2->python-keycloak<6.0.0,>=5.7.0->orkg) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.2->python-keycloak<6.0.0,>=5.7.0->orkg) (0.16.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4->Faker<20.0.0,>=19.1.0->orkg) (1.17.0)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.2->python-keycloak<6.0.0,>=5.7.0->orkg) (1.3.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas<3.0.0,>=2.0.1->orkg) (2024.2.0)\r\n",
      "Downloading orkg-1.0.1-py3-none-any.whl (56 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading Faker-19.13.0-py3-none-any.whl (1.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading Inflector-3.1.1-py3-none-any.whl (12 kB)\r\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_keycloak-5.8.1-py3-none-any.whl (77 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading undecorated-0.3.0-py3-none-any.whl (4.8 kB)\r\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\r\n",
      "Downloading async_property-0.2.2-py2.py3-none-any.whl (9.5 kB)\r\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Downloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: cardinality, hammock\r\n",
      "  Building wheel for cardinality (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for cardinality: filename=cardinality-0.1.1-py3-none-any.whl size=2587 sha256=9c2d04bb4f5f05387243ede345454872237afa18f682b7cace4ec098c8817e96\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/ab/4f/3b78b5eb5cb27aa555514c3f3858530a75e1c1f3491c3e0c30\r\n",
      "  Building wheel for hammock (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for hammock: filename=hammock-0.2.4-py3-none-any.whl size=2405 sha256=6dbca6d443daa7f51db74c5c89e5ed5fed8535ee9a7a2632edce7b786990cda4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/27/75/08dd4ad22c436de51da413c94c5978a579982a311da8b3fde7\r\n",
      "Successfully built cardinality hammock\r\n",
      "Installing collected packages: undecorated, Inflector, cardinality, async-property, pypdfium2, loguru, deprecation, aiofiles, hammock, Faker, pdfminer.six, jwcrypto, python-keycloak, pdfplumber, orkg\r\n",
      "  Attempting uninstall: aiofiles\r\n",
      "    Found existing installation: aiofiles 22.1.0\r\n",
      "    Uninstalling aiofiles-22.1.0:\r\n",
      "      Successfully uninstalled aiofiles-22.1.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "ypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed Faker-19.13.0 Inflector-3.1.1 aiofiles-24.1.0 async-property-0.2.2 cardinality-0.1.1 deprecation-2.1.0 hammock-0.2.4 jwcrypto-1.5.6 loguru-0.7.3 orkg-1.0.1 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0 python-keycloak-5.8.1 undecorated-0.3.0\r\n",
      "\u001b[33mWARNING: Skipping keycloak as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting python-keycloak==1.7.0\r\n",
      "  Downloading python_keycloak-1.7.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from python-keycloak==1.7.0) (2.32.4)\r\n",
      "Collecting python-jose>=1.4.0 (from python-keycloak==1.7.0)\r\n",
      "  Downloading python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from python-keycloak==1.7.0) (2.5.0)\r\n",
      "Collecting ecdsa!=0.15 (from python-jose>=1.4.0->python-keycloak==1.7.0)\r\n",
      "  Downloading ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\r\n",
      "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from python-jose>=1.4.0->python-keycloak==1.7.0) (4.9.1)\r\n",
      "Requirement already satisfied: pyasn1>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from python-jose>=1.4.0->python-keycloak==1.7.0) (0.6.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->python-keycloak==1.7.0) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->python-keycloak==1.7.0) (3.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->python-keycloak==1.7.0) (2025.6.15)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from ecdsa!=0.15->python-jose>=1.4.0->python-keycloak==1.7.0) (1.17.0)\r\n",
      "Downloading python_keycloak-1.7.0-py3-none-any.whl (39 kB)\r\n",
      "Downloading python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\r\n",
      "Downloading ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ecdsa, python-jose, python-keycloak\r\n",
      "  Attempting uninstall: python-keycloak\r\n",
      "    Found existing installation: python-keycloak 5.8.1\r\n",
      "    Uninstalling python-keycloak-5.8.1:\r\n",
      "      Successfully uninstalled python-keycloak-5.8.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "orkg 1.0.1 requires python-keycloak<6.0.0,>=5.7.0, but you have python-keycloak 1.7.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed ecdsa-0.19.1 python-jose-3.5.0 python-keycloak-1.7.0\r\n",
      "\n",
      "Successfull dependance installation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score json-repair\n",
    "!pip install orkg requests_toolbelt urllib3 pdfplumber\n",
    "!pip uninstall -y keycloak\n",
    "!pip install python-keycloak==1.7.0\n",
    "print(\"\\nSuccessfull dependance installation\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d624b7",
   "metadata": {
    "_cell_guid": "352e095a-6923-45e3-8089-490825000204",
    "_uuid": "3ac45723-df34-49dd-a219-eccd20f9ddd1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009211,
     "end_time": "2025-10-09T13:04:05.371988",
     "exception": false,
     "start_time": "2025-10-09T13:04:05.362777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Disable Wandb reporting during finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1463101b",
   "metadata": {
    "_cell_guid": "6bf43df6-d22d-44ea-b44f-e6288022df05",
    "_uuid": "76a581f6-e248-4839-b11d-330ea37c8264",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-09T13:04:05.391902Z",
     "iopub.status.busy": "2025-10-09T13:04:05.391623Z",
     "iopub.status.idle": "2025-10-09T13:04:05.396108Z",
     "shell.execute_reply": "2025-10-09T13:04:05.395297Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015802,
     "end_time": "2025-10-09T13:04:05.397243",
     "exception": false,
     "start_time": "2025-10-09T13:04:05.381441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disable wandb option\n"
     ]
    }
   ],
   "source": [
    "# Disable wand db option\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "print(\"Disable wandb option\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76a4b0",
   "metadata": {
    "_cell_guid": "64f28fed-ab6c-4e80-b189-5c6059351d7e",
    "_uuid": "a3b9b711-7b88-479b-b408-5935d720c49f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0097,
     "end_time": "2025-10-09T13:04:05.416901",
     "exception": false,
     "start_time": "2025-10-09T13:04:05.407201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define prompt template: \n",
    "- Zero-shot\n",
    "- Few-shot\n",
    "- Role-prompting\n",
    "- Contextual prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c51e5c1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T13:04:05.437349Z",
     "iopub.status.busy": "2025-10-09T13:04:05.437127Z",
     "iopub.status.idle": "2025-10-09T13:04:05.452375Z",
     "shell.execute_reply": "2025-10-09T13:04:05.451646Z"
    },
    "papermill": {
     "duration": 0.027244,
     "end_time": "2025-10-09T13:04:05.453519",
     "exception": false,
     "start_time": "2025-10-09T13:04:05.426275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts.json généré avec 24 versions\n",
      "\n",
      "Prompt Loader function defined successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# progression des champs\n",
    "schemas = [\n",
    "    {\"local name\": \"string\"},\n",
    "    {\"local name\": \"string\", \"common name\": \"string\"},\n",
    "    {\"local name\": \"string\", \"common name\": \"string\", \"geographical area\": \"string\"},\n",
    "    {\"local name\": \"string\", \"common name\": \"string\", \"geographical area\": \"string\", \"food group\": \"string\"},\n",
    "    {\n",
    "        \"local name\": \"string\",\n",
    "        \"common name\": \"string\",\n",
    "        \"geographical area\": \"string\",\n",
    "        \"food group\": \"string\",\n",
    "        \"ingredients\": [\n",
    "            {\"common name\": \"string\", \"scientific name\": \"string\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"geographical area\": \"string\",\n",
    "        \"local name\": \"string\",\n",
    "        \"common name\": \"string\",\n",
    "        \"food group\": \"string\",\n",
    "        \"ingredients\": [\n",
    "            {\"common name\": \"string\", \"scientific name\": \"string\"}\n",
    "        ],\n",
    "        \"food components\": [\n",
    "            {\"label\": \"string\", \"value\": \"float\", \"unit\": \"string\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# exemples pour few-shot\n",
    "examples = [\n",
    "    {\n",
    "        \"text\": \"\"\"CONDENSED FOOD COMPOSITION TABLE | TABLE DE COMPOSITION DES ALIMENTS CONDENSÉE\\n423\\nFOOD FOOD NAME CHOLESTEROL SAT FA MONOUNSAT FA POLYUNSAT FA LINOLEIC ACID ALPHA-LINOLENIC ACID PHYTATE\\nCODE IN ENGLISH (mg) (g) (g) (g) (g) (g) (mg)\\nINFOODS TAGNAMES CHOLE(mg) FASAT(g) FAMS(g) FAPU(g) F18D2CN6(g) F18D3CN3(g) PHYTCPP(mg)\\nSoups and sauces/Soupes et sauces\\nNayoungn vand zindo (Burkina Faso)*: sweet potato leaf\\n14_009 8 6.04 4.52 1.40 [1.20] [0.04] [2]\\nsauce, fish, red palm oil and vegetables\\nPésgo né tomate zéindo (Burkina Faso)*: sauce with lamb,\\n14_002 19 4.41 5.64 5.12 4.87 0.16 [3]\\ntomato and vegetables\\nSiikam zéédo (Burkina Faso)*: groundnut sauce with\\n14_006 6 3.44 4.25 2.22 2.05 0.05 [33]\\nvegetables, fish and fermented African locust beans\\nSiné zindo (Burkina Faso)*: sesame sauce with fish and\\n14_028 8 2.77 5.87 8.24 7.99 0.07 [135]\\nvegetables\\nSoup from chicken, beer yeast, vegetables and fermented\\n14_033 11 1.08 2.03 1.95 1.90 0.04 [1]\\nAfrican locust beans (Burkina Faso)*\\n14_034 Soup with cabbage and vegetables (Burkina Faso)* 0 0.04 0.03 0.04 0.03 0.01 [3]\\nTéi kam zéédo (Burkina Faso)*: red palm oil sauce with\\n14_005 7 2.95 3.94 2.11 1.95 0.02 [33]\\nfish and vegetables\\nTéi né maane zéindo (Burkina Faso)*: oil palm fruit sauce\\n14_024 6 2.77 2.83 0.80 0.09 0.01 [1]\\nwith okra powder, fish and vegetables\\nTéi zéindo (Burkina Faso)*: simple oil palm fruit sauce with\\n14_023 3 6.47 6.85 1.69 0.18 0.01 [0]\\nfish and vegetables\\nToeg koing zéédo (Burkina Faso)*: dried baobab leaf sauce\\n14_020 5 0.50 0.69 1.09 0.95 0.04 [1]\\nwith fish, vegetables and fermented African locust beans\\nToeg maas zéédo (Burkina Faso)*: fresh baobab leaf sauce\\n14_022 6 0.74 1.20 1.87 1.71 0.04 [7]\\nwith fish, vegetables and fermented African locust beans\\n14_010 Yassa sauce with lamb, onion and mustard (Burkina Faso)* 10 4.09 7.00 8.46 8.21 0.19 [3]\"\"\",\n",
    "        \"output\": \"\"\"[{\"geographical area\": \"West african\", \"local name\": \"Onion\", \"common name\": null, \"food group\": \"Vegetables and their products\", \"food components\": [{\"label\": \"Onion,raw01_energy\", \"value\": 37.0, \"unit\": \"Kcal\"}, {\"label\": \"Onion,raw01_water\", \"value\": 89.5, \"unit\": \"g\"}, {\"label\": \"Onion,raw01_protein\", \"value\": 1.1, \"unit\": \"g\"}, {\"label\": \"Onion,raw01_fat\", \"value\": 0.1, \"unit\": \"g\"}, {\"label\": \"Onion,raw01_carbohydrate-available\", \"value\": 6.9, \"unit\": \"g\"}, {\"label\": \"Onion,raw01_fibre\", \"value\": 1.8, \"unit\": \"g\"}, {\"label\": \"Onion,raw01_ash\", \"value\": 0.6, \"unit\": \"g\"}, {\"label\": \"Onion,raw01_calcium\", \"value\": 25.0, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_iron\", \"value\": 0.3, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_magnesium\", \"value\": 10.0, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_phosphorus\", \"value\": 39.0, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_potassium\", \"value\": 183.0, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_sodium\", \"value\": 4.0, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_zinc\", \"value\": 0.26, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_copper\", \"value\": 0.04, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_b carotene equivalent\", \"value\": 1.0, \"unit\": \"mcg\"}, {\"label\": \"Onion,raw01_vitamin E\", \"value\": 0.04, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_thiamin\", \"value\": 0.05, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_riboflavin\", \"value\": 0.04, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_niacin\", \"value\": 0.2, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_vitamin B6\", \"value\": 0.1, \"unit\": \"mg\"}, {\"label\": \"Onion,raw01_folate\", \"value\": 16.0, \"unit\": \"mcg\"}, {\"label\": \"Onion,raw01_vitamin C\", \"value\": 10.3, \"unit\": \"mg\"}]}, {\"geographical area\": \"West african\", \"local name\": \"Onion\", \"common name\": null, \"food group\": \"Vegetables and their products\", \"food components\": [{\"label\": \"Onion,boiled01_energy\", \"value\": 45.0, \"unit\": \"Kcal\"}, {\"label\": \"Onion,boiled01_water\", \"value\": 87.2, \"unit\": \"g\"}, {\"label\": \"Onion,boiled01_protein\", \"value\": 1.4, \"unit\": \"g\"}, {\"label\": \"Onion,boiled01_fat\", \"value\": 0.1, \"unit\": \"g\"}, {\"label\": \"Onion,boiled01_carbohydrate-available\", \"value\": 8.4, \"unit\": \"g\"}, {\"label\": \"Onion,boiled01_fibre\", \"value\": 2.2, \"unit\": \"g\"}, {\"label\": \"Onion,boiled01_ash\", \"value\": 0.8, \"unit\": \"g\"}, {\"label\": \"Onion,boiled01_calcium\", \"value\": 29.0, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_iron\", \"value\": 0.3, \"unit\": \"g\"}, {\"label\": \"Onion,boiled01_magnesium\", \"value\": 7.0, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_phosphorus\", \"value\": 43.0, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_potassium\", \"value\": 101.0, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_sodium\", \"value\": 3.0, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_zinc\", \"value\": 0.24, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_copper\", \"value\": 0.05, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_b ccarotene equivalent\", \"value\": 1.0, \"unit\": \"mcg\"}, {\"label\": \"Onion,boiled01_vitamin E\", \"value\": 0.05, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_thiamin\", \"value\": 0.04, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_riboflavin\", \"value\": 0.04, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_niacin\", \"value\": 0.2, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_vitamin B6\", \"value\": 0.08, \"unit\": \"mg\"}, {\"label\": \"Onion,boiled01_folate\", \"value\": 10.0, \"unit\": \"mcg\"}, {\"label\": \"Onion,boiled01_vitamin C\", \"value\": 4.2, \"unit\": \"mg\"}]}, {\"geographical area\": \"West african\", \"local name\": \"Onion\", \"common name\": null, \"food group\": \"Vegetables and their products\", \"food components\": [{\"label\": \"Onion,died01_energy\", \"value\": 354.0, \"unit\": \"Kcal\"}, {\"label\": \"Onion,died01_water\", \"value\": 4.2, \"unit\": \"g\"}, {\"label\": \"Onion,died01_protein\", \"value\": 9.6, \"unit\": \"g\"}, {\"label\": \"Onion,died01_fat\", \"value\": 1.1, \"unit\": \"g\"}, {\"label\": \"Onion,died01_carbohydrate-available\", \"value\": 71.1, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_fibre\", \"value\": 10.7, \"unit\": \"g\"}, {\"label\": \"Onion,died01_ash\", \"value\": 3.4, \"unit\": \"g\"}, {\"label\": \"Onion,died01_calcium\", \"value\": 239.0, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_iron\", \"value\": 2.1, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_magnesium\", \"value\": 64.0, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_phosphorus\", \"value\": 282.0, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_potassium\", \"value\": 1510.0, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_sodium\", \"value\": 21.0, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_zinc\", \"value\": 1.8, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_copper\", \"value\": 0.4, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_vitamin A-RAE\", \"value\": 1.0, \"unit\": \"mcg\"}, {\"label\": \"Onion,died01_b carotene equivalent\", \"value\": 11.0, \"unit\": \"mcg\"}, {\"label\": \"Onion,died01_vitamin E\", \"value\": 0.4, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_thiamin\", \"value\": 0.37, \"unit\": \"g\"}, {\"label\": \"Onion,died01_riboflavin\", \"value\": 0.1, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_niacin\", \"value\": 0.9, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_vitamin B6\", \"value\": 1.6, \"unit\": \"mg\"}, {\"label\": \"Onion,died01_folate\", \"value\": 166.0, \"unit\": \"mcg\"}, {\"label\": \"Onion,died01_vitamin C\", \"value\": 54.0, \"unit\": \"mg\"}]}]\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "prompts = {\"prompts\": []}\n",
    "\n",
    "# V1 → V6 : zero-shot\n",
    "for i, schema in enumerate(schemas, start=1):\n",
    "    prompts[\"prompts\"].append({\n",
    "        \"version\": i,\n",
    "        \"mode\": \"zero-shot\",\n",
    "        \"instruction\": \"Extract all food contributions mentioned in the following raw tabular data text. Each contribution must be returned as a valid JSON object with the following structure:\",\n",
    "        \"schema\": schema\n",
    "    })\n",
    "\n",
    "# V7 → V12 : few-shot\n",
    "for i, schema in enumerate(schemas, start=7):\n",
    "    prompts[\"prompts\"].append({\n",
    "        \"version\": i,\n",
    "        \"mode\": \"few-shot\",\n",
    "        \"instruction\": \"Extract all food contributions from raw tabular data text using the examples below as guidance.\",\n",
    "        \"examples\": examples,\n",
    "        \"schema\": schema\n",
    "    })\n",
    "\n",
    "# V13 → V24 : role prompting\n",
    "for i, schema in enumerate(schemas * 2, start=13):  # répète 6 fois zero-shot + 6 fois few-shot\n",
    "    mode = \"role-zero-shot\" if i < 19 else \"role-few-shot\"\n",
    "    entry = {\n",
    "        \"version\": i,\n",
    "        \"mode\": mode,\n",
    "        \"role\": \"You are an expert in food composition analysis.\",\n",
    "        \"schema\": schema\n",
    "    }\n",
    "    if \"few-shot\" in mode:\n",
    "        entry[\"instruction\"] = \"Your task is to extract all food contributions from the raw tabular data text using the examples below as guidance.\"\n",
    "        entry[\"examples\"] = examples\n",
    "    else:\n",
    "        entry[\"instruction\"] = \"Your task is to extract all food contributions mentioned in the following raw tabular data text. Return them as valid JSON objects with the following structure:\"\n",
    "    prompts[\"prompts\"].append(entry)\n",
    "\n",
    "# Sauvegarde du fichier\n",
    "with open(\"/kaggle/working/prompts.json\", \"w\") as f:\n",
    "    json.dump(prompts, f, indent=2)\n",
    "\n",
    "print(\"prompts.json généré avec 24 versions\")\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def load_prompt(version: int, input_text: str, path: str = \"/kaggle/working/prompts.json\") -> str:\n",
    "    # Charger le fichier\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Chercher la version\n",
    "    prompt_data = next((p for p in data[\"prompts\"] if p[\"version\"] == version), None)\n",
    "    if prompt_data is None:\n",
    "        raise ValueError(f\"Version {version} not found in {path}\")\n",
    "\n",
    "    # Construire le corps du prompt\n",
    "    base = \"\"\n",
    "\n",
    "    # Ajouter rôle si présent\n",
    "    if \"role\" in prompt_data:\n",
    "        base += prompt_data[\"role\"] + \"\\n\\n\"\n",
    "\n",
    "    # Ajouter instruction\n",
    "    base += prompt_data[\"instruction\"] + \"\\n\\n\"\n",
    "\n",
    "    # Ajouter exemples si présents (few-shot ou role-few-shot)\n",
    "    if \"examples\" in prompt_data:\n",
    "        base += \"Examples:\\n\"\n",
    "        for ex in prompt_data[\"examples\"]:\n",
    "            base += f\"Text: {ex['text']}\\nOutput: {json.dumps(ex['output'], indent=2)}\\n\\n\"\n",
    "\n",
    "    # Ajouter schema\n",
    "    base += \"Schema:\\n\" + json.dumps(prompt_data[\"schema\"], indent=2) + \"\\n\\n\"\n",
    "\n",
    "    # Ajouter le texte cible\n",
    "    base += f\"Text:\\n{input_text}\\n\\nJSON array:\"\n",
    "\n",
    "    return base\n",
    "\n",
    "print('\\nPrompt Loader function defined successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07c742e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T13:04:05.473313Z",
     "iopub.status.busy": "2025-10-09T13:04:05.473138Z",
     "iopub.status.idle": "2025-10-09T13:04:05.477937Z",
     "shell.execute_reply": "2025-10-09T13:04:05.477268Z"
    },
    "papermill": {
     "duration": 0.015927,
     "end_time": "2025-10-09T13:04:05.479027",
     "exception": false,
     "start_time": "2025-10-09T13:04:05.463100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract all food contributions mentioned in the following raw tabular data text. Each contribution must be returned as a valid JSON object with the following structure:\n",
      "\n",
      "Schema:\n",
      "{\n",
      "  \"geographical area\": \"string\",\n",
      "  \"local name\": \"string\",\n",
      "  \"common name\": \"string\",\n",
      "  \"food group\": \"string\",\n",
      "  \"ingredients\": [\n",
      "    {\n",
      "      \"common name\": \"string\",\n",
      "      \"scientific name\": \"string\"\n",
      "    }\n",
      "  ],\n",
      "  \"food components\": [\n",
      "    {\n",
      "      \"label\": \"string\",\n",
      "      \"value\": \"float\",\n",
      "      \"unit\": \"string\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Text:\n",
      "myText\n",
      "\n",
      "JSON array:\n"
     ]
    }
   ],
   "source": [
    "prompt_test = load_prompt(6, \"myText\")\n",
    "print(prompt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae67f67",
   "metadata": {
    "_cell_guid": "0c8e8062-c3e2-4e0d-ae40-bc6fa160b2d0",
    "_uuid": "5237052a-12f3-474e-9a0c-c4c4fdd51cf9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-09T13:04:05.499583Z",
     "iopub.status.busy": "2025-10-09T13:04:05.499410Z",
     "iopub.status.idle": "2025-10-09T13:04:05.504694Z",
     "shell.execute_reply": "2025-10-09T13:04:05.504186Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016781,
     "end_time": "2025-10-09T13:04:05.505638",
     "exception": false,
     "start_time": "2025-10-09T13:04:05.488857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utils: Get prompt template\n",
    "\n",
    "# def get_prompt(input_text):\n",
    "#     # input_text = \" \".join(input_text)  # fusionne les segments s’il y en a plusieurs\n",
    "#     input_text = input_text.strip()\n",
    "\n",
    "\n",
    "#     # === Prompt 1 : Zero-shot ===\n",
    "#     zero_shot = f\"\"\"Extract all food contributions mentioned in the following text.\n",
    "\n",
    "#         Each contribution must be returned as a JSON object with the following structure:\n",
    "        \n",
    "#         {{\n",
    "#           \"geographical area\": \"string\",\n",
    "#           \"local name\": \"string\",\n",
    "#           \"common name\": \"string\",\n",
    "#           \"food group\": \"string\",\n",
    "#           \"ingredients\": [\n",
    "#             {{\n",
    "#               \"common name\": \"string\",\n",
    "#               \"scientific name\": \"string\"\n",
    "#             }}\n",
    "#           ],\n",
    "#           \"food components\": [\n",
    "#             {{\n",
    "#               \"label\": \"string\",\n",
    "#               \"value\": float,\n",
    "#               \"unit\": \"string\"\n",
    "#             }}\n",
    "#           ]\n",
    "#         }}\n",
    "\n",
    "#         If a field is not present in the text or you're not sure about its value, set it to `unanswerable`. Do not guess or hallucinate.\n",
    "        \n",
    "#         Return a JSON array containing all contributions.\n",
    "        \n",
    "#         Text:\n",
    "#         {input_text}\n",
    "\n",
    "#         JSON array:\n",
    "#         \"\"\"\n",
    "\n",
    "#     # === Prompt 2 : Few-shot ===\n",
    "#     few_shot = f\"\"\"You are an expert food knowledge extraction model.\n",
    "\n",
    "#         You will read a text and extract a list of food contributions. Each contribution must follow this structure:\n",
    "        \n",
    "#         {{\n",
    "#           \"geographical area\": \"string\",\n",
    "#           \"local name\": \"string\",\n",
    "#           \"common name\": \"string\",\n",
    "#           \"food group\": \"string\",\n",
    "#           \"ingredients\": [\n",
    "#             {{\n",
    "#               \"common name\": \"string\",\n",
    "#               \"scientific name\": \"string\"\n",
    "#             }}\n",
    "#           ],\n",
    "#           \"food components\": [\n",
    "#             {{\n",
    "#               \"label\": \"string\",\n",
    "#               \"value\": float,\n",
    "#               \"unit\": \"string\"\n",
    "#             }}\n",
    "#           ]\n",
    "#         }}\n",
    "\n",
    "#         If a field is not present in the text or you're not sure about its value, set it to `unanswerable`. Do not guess or hallucinate.\n",
    "        \n",
    "#         Return a valid JSON array containing one or more contributions.\n",
    "        \n",
    "#         Example 1:\n",
    "#         Text:\n",
    "#         Cassava is boiled and served with palm oil. It contains Vitamin C (14 mg) and Water (60%).\n",
    "        \n",
    "#         Answer:\n",
    "#         [\n",
    "#           {{\n",
    "#             \"common_name\": \"Cassava\",\n",
    "#             \"food_form\": \"Boiled\",\n",
    "#             \"food_name\": \"Boiled cassava\",\n",
    "#             \"ingredients\": [{{ \"common_name\": \"Palm oil\", \"scientific_name\": null }}],\n",
    "#             \"food_components\": [\n",
    "#               {{ \"label\": \"Vitamin C\", \"value\": 14.0, \"unit\": \"mg\" }},\n",
    "#               {{ \"label\": \"Water\", \"value\": 60.0, \"unit\": \"%\" }}\n",
    "#             ]\n",
    "#           }}\n",
    "#         ]\n",
    "        \n",
    "#         Example 2:\n",
    "#         Text:\n",
    "#         Roasted maize is eaten with coconut. It contains Energy (250 kcal).\n",
    "        \n",
    "#         Answer:\n",
    "#         [\n",
    "#           {{\n",
    "#             \"common_name\": \"Maize\",\n",
    "#             \"food_form\": \"Roasted\",\n",
    "#             \"food_name\": \"Roasted maize with coconut\",\n",
    "#             \"ingredients\": [{{ \"common_name\": \"Coconut\", \"scientific_name\": null }}],\n",
    "#             \"food_components\": [\n",
    "#               {{ \"label\": \"Energy\", \"value\": 250.0, \"unit\": \"kcal\" }}\n",
    "#             ]\n",
    "#           }}\n",
    "#         ]\n",
    "        \n",
    "#         Now extract the contributions from the following text:\n",
    "        \n",
    "#         Text:\n",
    "#         {input_text}\n",
    "\n",
    "#         Answer:\n",
    "#         \"\"\"\n",
    "\n",
    "#     # === Prompt 3 : Role prompting ===\n",
    "#     role_prompt = f\"\"\"You are a food composition scientist specialized in extracting structured data from articles.\n",
    "\n",
    "#         Your task is to read a text and extract a list of food contributions. Each contribution must follow this JSON format:\n",
    "        \n",
    "#         {{\n",
    "#           \"geographical area\": \"string\",\n",
    "#           \"local name\": \"string\",\n",
    "#           \"common name\": \"string\",\n",
    "#           \"food group\": \"string\",\n",
    "#           \"ingredients\": [\n",
    "#             {{\n",
    "#               \"common name\": \"string\",\n",
    "#               \"scientific name\": \"string\"\n",
    "#             }}\n",
    "#           ],\n",
    "#           \"food components\": [\n",
    "#             {{\n",
    "#               \"label\": \"string\",\n",
    "#               \"value\": float,\n",
    "#               \"unit\": \"string\"\n",
    "#             }}\n",
    "#           ]\n",
    "#         }}\n",
    "\n",
    "#         If a field is not present in the text or you're not sure about its value, set it to `unanswerable`. Do not guess or hallucinate.\n",
    "        \n",
    "#         Make sure to output a valid JSON array.\n",
    "        \n",
    "#         Text:\n",
    "#         {input_text}\n",
    "\n",
    "#         JSON Array:\n",
    "#         \"\"\"\n",
    "\n",
    "#     # === Prompt 4 : Contextual prompting ===\n",
    "#     contextual = f\"\"\"Context:\n",
    "#         You are analyzing a scientific document describing foods composition tables. Your goal is to extract structured food composition data for use in a food knowledge graph.\n",
    "        \n",
    "#         Expected output format:\n",
    "#         Each contribution must be returned as a JSON object with this structure:\n",
    "        \n",
    "#         {{\n",
    "#           \"geographical area\": \"string\",\n",
    "#           \"local name\": \"string\",\n",
    "#           \"common name\": \"string\",\n",
    "#           \"food group\": \"string\",\n",
    "#           \"ingredients\": [\n",
    "#             {{\n",
    "#               \"common name\": \"string\",\n",
    "#               \"scientific name\": \"string\"\n",
    "#             }}\n",
    "#           ],\n",
    "#           \"food components\": [\n",
    "#             {{\n",
    "#               \"label\": \"string\",\n",
    "#               \"value\": float,\n",
    "#               \"unit\": \"string\"\n",
    "#             }}\n",
    "#           ]\n",
    "#         }}\n",
    "\n",
    "#         If a field is not present in the text or you're not sure about its value, set it to `unanswerable`. Do not guess or hallucinate.\n",
    "        \n",
    "#         Return a JSON array of one or more contributions.\n",
    "        \n",
    "#         Text:\n",
    "#         {input_text}\n",
    "\n",
    "#         JSON array:\n",
    "#         \"\"\"\n",
    "\n",
    "#     # Choix aléatoire\n",
    "#     prompts = [zero_shot, few_shot, role_prompt, contextual]\n",
    "#     # return random.choice(prompts)\n",
    "#     return zero_shot\n",
    "\n",
    "# print(\"\\nget_prompt defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1219c84",
   "metadata": {
    "_cell_guid": "1ae7941e-61a3-4847-a5fe-7aa6ce77b3d9",
    "_uuid": "5a3ecf58-6c47-445a-b4a1-f75831fb1974",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-09T13:04:05.525608Z",
     "iopub.status.busy": "2025-10-09T13:04:05.525436Z",
     "iopub.status.idle": "2025-10-09T13:04:46.250360Z",
     "shell.execute_reply": "2025-10-09T13:04:46.249493Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 40.746687,
     "end_time": "2025-10-09T13:04:46.261735",
     "exception": false,
     "start_time": "2025-10-09T13:04:05.515048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 13:04:26.332184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760015066.711582      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760015066.827479      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation method defined successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "\n",
    "import evaluate\n",
    "import numpy\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "numpy.random.seed(42)\n",
    "unanswerable = 'unanswerable'\n",
    "feature_names = [\"geoographical area\", \"local name\", \"common name\", \"food group\", \"ingredients\", \"food components\"]\n",
    "\n",
    "class MatchType(Enum):\n",
    "    EXACT = \"EXACT\"\n",
    "    PARTIAL = \"PARTIAL\"\n",
    "\n",
    "\n",
    "def is_answerable(text):\n",
    "    return str(text).strip().lower() != unanswerable\n",
    "\n",
    "\n",
    "def is_unanswerable(text):\n",
    "    return str(text).strip().lower() == unanswerable\n",
    "\n",
    "\n",
    "def calculate_fuzz_ratio(text1, text2):\n",
    "    return fuzz.ratio(str(text1).strip().lower(), str(text2).strip().lower())\n",
    "\n",
    "\n",
    "def get_contribution_feature_values_list(contribution):\n",
    "    return str(contribution).strip().split(\"\\n\")\n",
    "\n",
    "\n",
    "def is_partial_match_json_based(label_feature_value, prediction_feature_value):\n",
    "    # print(\"\\nis_partial_match_json_based\\n\")\n",
    "    # print(f\"label_feature_value: {label_feature_value}\")\n",
    "    # print(f\"prediction_feature_value: {prediction_feature_value}\\n\")\n",
    "    # print(f\"fuzzy ration: {calculate_fuzz_ratio(label_feature_value, prediction_feature_value)}\\n\")\n",
    "    \n",
    "    return label_feature_value != '-' and prediction_feature_value != '-' and label_feature_value != \"\" and prediction_feature_value != \"\" \\\n",
    "        and calculate_fuzz_ratio(label_feature_value, prediction_feature_value) >= 80\n",
    "\n",
    "\n",
    "def is_exact_match_json_based(label_feature_value, prediction_feature_value):\n",
    "    return label_feature_value != '-' and prediction_feature_value != '-' \\\n",
    "        and label_feature_value != \"\" and prediction_feature_value != \"\" \\\n",
    "        and str(label_feature_value).strip().lower() == str(prediction_feature_value).strip().lower()\n",
    "\n",
    "\n",
    "def get_number_of_values_of_feature_json_based(feature_name, json_item_list):\n",
    "    number_of_features_with_value = sum(\n",
    "        [1 for contribution in json_item_list if has_feature_value_json_based(contribution, feature_name)])\n",
    "    # print(\"in this text \", feature_name, \" has \", number_of_features_with_value, \" occurrence with values \")\n",
    "    return number_of_features_with_value\n",
    "\n",
    "def has_feature_value_json_based(contribution, feature_name):\n",
    "    contribution_root = get_contribution_root(contribution)\n",
    "    feature_value = get_feature_value_json_based(contribution_root, feature_name)\n",
    "    # contribution_root = contribution.get(\"contribution\", {}) if isinstance(contribution, dict) else {}\n",
    "    # feature_value = contribution_root.get(feature_name, {}) if isinstance(contribution_root, dict) else {}\n",
    "    return feature_value != '-' and feature_value != \"\"\n",
    "\n",
    "\n",
    "def get_feature_value_json_based(contribution_root, feature_name):\n",
    "    # feature_value = \"\"\n",
    "    # if isinstance(contribution_root, dict):\n",
    "    #     for key in contribution_root.keys():\n",
    "    #         if key.strip().lower() == feature_name.lower():\n",
    "    #             feature_value = str(contribution_root[key]).strip().lower()\n",
    "    #             break\n",
    "    # return feature_value\n",
    "    feature_value = \"\"\n",
    "    if not isinstance(contribution_root, dict):\n",
    "        return feature_value\n",
    "\n",
    "    for key in contribution_root.keys():\n",
    "        if key.strip().lower() == feature_name.lower():\n",
    "            value = contribution_root[key]\n",
    "\n",
    "            # Si c’est une liste de dicts, on convertit en JSON trié\n",
    "            if isinstance(value, list) and all(isinstance(x, dict) for x in value):\n",
    "                feature_value = json.dumps(value, sort_keys=True)\n",
    "            else:\n",
    "                feature_value = str(value).strip().lower()\n",
    "            break\n",
    "\n",
    "    return feature_value\n",
    "\n",
    "\n",
    "def get_contribution_root(contribution):\n",
    "    \"\"\"\n",
    "    Pour ta structure, on considère que le dictionnaire de contribution\n",
    "    est déjà à la racine. Donc on retourne directement l'objet s'il est un dict.\n",
    "    \"\"\"\n",
    "    return contribution if isinstance(contribution, dict) else {}\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# def extract_label_prediction_feature_values_json(feature_name, label_contribution, prediction_contribution):\n",
    "#     label_feature_value = label_contribution.get(\"contribution\", {}).get(feature_name, \"\") if isinstance(\n",
    "#         label_contribution, dict) else \"\"\n",
    "#     prediction_contribution_root = get_contribution_root(contribution=prediction_contribution)\n",
    "#     prediction_feature_value = get_feature_value_json_based(contribution_root=prediction_contribution_root,\n",
    "#                                                             feature_name=feature_name)\n",
    "#     return label_feature_value, prediction_feature_value\n",
    "def extract_label_prediction_feature_values_json(feature_name,label_contribution, prediction_contribution ):\n",
    "    # print(f\"label contribution: {label_contribution}\")\n",
    "    if not isinstance(label_contribution, dict) or not isinstance(prediction_contribution, dict):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    if feature_name in [\"geographical area\", \"local name\", \"common name\", \"food group\"]:\n",
    "        label_value = label_contribution.get(feature_name, \"\").lower()\n",
    "        prediction_value = prediction_contribution.get(feature_name, \"\").lower()\n",
    "\n",
    "    elif feature_name == \"ingredients\":\n",
    "        label_ingredients = label_contribution.get(\"ingredients\", [])\n",
    "        prediction_ingredients = prediction_contribution.get(\"ingredients\", [])\n",
    "        label_value = sorted([i.get(\"common name\", \"\").lower() for i in label_ingredients])\n",
    "        prediction_value = sorted([i.get(\"common name\", \"\").lower() for i in prediction_ingredients])\n",
    "\n",
    "    elif feature_name == \"food components\":\n",
    "        label_components = label_contribution.get(\"food components\", [])\n",
    "        prediction_components = prediction_contribution.get(\"food components\", [])\n",
    "        label_value = sorted(\n",
    "            [(c.get(\"label\", \"\").lower(), float(c.get(\"value\", 0)), c.get(\"unit\", \"\").lower()) for c in label_components]\n",
    "        )\n",
    "        prediction_value = sorted(\n",
    "            [(c.get(\"label\", \"\").lower(), float(c.get(\"value\", 0)), c.get(\"unit\", \"\").lower()) for c in prediction_components]\n",
    "        )\n",
    "    else:\n",
    "        label_value, prediction_value = \"\", \"\"\n",
    "\n",
    "    return label_value, prediction_value\n",
    "    \n",
    "\n",
    "def compute_exact_score_between_2_contributions_json_based(prediction_contribution, label_contribution, feature_name):\n",
    "\n",
    "    # print(\"compute_exact_score_between_2_contributions_json_based - INPUT \\n\")\n",
    "    # print(\"prediction_contribution ----\")\n",
    "    # print(prediction_contribution)\n",
    "    # print(\"\\n\\n\")\n",
    "\n",
    "    # print(\"label_contribution ----\")\n",
    "    # print(label_contribution)\n",
    "    # print(\"\\n\\n\")\n",
    "    \n",
    "    label_feature_value, prediction_feature_value = extract_label_prediction_feature_values_json(feature_name,\n",
    "                                                                                                 label_contribution,\n",
    "                                                                                                 prediction_contribution)\n",
    "    # print(\"\\ncompute_exact_score_between_2_contributions_json_based ------\\n\")\n",
    "    # print(f\"feature: {feature_name}\")\n",
    "    # print(f\"label value: {label_feature_value}\")\n",
    "    # print(f\"prediction value: {prediction_feature_value} \\n\\n\")\n",
    "\n",
    "    if is_exact_match_json_based(label_feature_value=label_feature_value,\n",
    "                                 prediction_feature_value=prediction_feature_value):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def compute_partial_score_between_2_contributions_json_based(prediction_contribution, label_contribution, feature_name):\n",
    "    label_feature_value, prediction_feature_value = extract_label_prediction_feature_values_json(feature_name,\n",
    "                                                                                                 label_contribution,\n",
    "                                                                                                 prediction_contribution)\n",
    "    if is_partial_match_json_based(label_feature_value=label_feature_value,\n",
    "                                   prediction_feature_value=prediction_feature_value):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def make_list_of_pairs_json_based(label_list, prediction_list):\n",
    "    # make list of (label,prediction,similarity)\n",
    "    list_of_label_prediction_pairs = []\n",
    "    for label, prediction in zip(label_list, prediction_list):\n",
    "        pair_list = []\n",
    "        label_contribution_list = get_contribution_list_json_based(label)\n",
    "        prediction_contribution_list = get_contribution_list_json_based(prediction)\n",
    "        for item1 in label_contribution_list:\n",
    "            for item2 in prediction_contribution_list:\n",
    "                item1_str = item1\n",
    "                item2_str = item2\n",
    "                if isinstance(item1, dict):\n",
    "                    item1_str = json.dumps(item1)\n",
    "                if isinstance(item2, dict):\n",
    "                    item2_str = json.dumps(item2)\n",
    "\n",
    "                pair_list.append((item1, item2, calculate_fuzz_ratio(item1_str, item2_str)))\n",
    "\n",
    "        max_selectable_pairs = min(len(label_contribution_list), len(prediction_contribution_list))\n",
    "        top_similar_pairs = sorted(pair_list, key=lambda x: x[2], reverse=True)[:max_selectable_pairs]\n",
    "        list_of_label_prediction_pairs.extend(top_similar_pairs)\n",
    "\n",
    "    return list_of_label_prediction_pairs\n",
    "\n",
    "\n",
    "def get_contribution_list_json_based(text):\n",
    "    contribution_list = parse_json_string(text)\n",
    "    if not isinstance(contribution_list, list):\n",
    "        contribution_list = [contribution_list]\n",
    "\n",
    "    return contribution_list\n",
    "    # if isinstance(text, str):\n",
    "    #     text = parse_json_string(text)\n",
    "    # if not isinstance(text, list):\n",
    "    #     return [text]\n",
    "    # return text\n",
    "\n",
    "\n",
    "def get_feature_based_denominator_count_json_based(feature_name, item_list):\n",
    "    result = 0\n",
    "    for item in item_list:\n",
    "        json_item_list = get_contribution_list_json_based(item)\n",
    "        if is_answerable(json_item_list[0]):\n",
    "            result += get_number_of_values_of_feature_json_based(feature_name, json_item_list)\n",
    "    return result\n",
    "\n",
    "def calculate_feature_based_tp_json_based(pair_list, feature_name, mode):\n",
    "    result = 0\n",
    "    for pair in pair_list:\n",
    "        label, prediction = pair[0], pair[1]\n",
    "        if mode == MatchType.EXACT.value:\n",
    "            result += compute_exact_score_between_2_contributions_json_based(prediction_contribution=prediction,\n",
    "                                                                             label_contribution=label,\n",
    "                                                                             feature_name=feature_name)\n",
    "        if mode == MatchType.PARTIAL.value:\n",
    "            result += compute_partial_score_between_2_contributions_json_based(prediction_contribution=prediction,\n",
    "                                                                               label_contribution=label,\n",
    "                                                                               feature_name=feature_name)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_result_dict(exact_f1, exact_precision, exact_recalls, general_accuracy, partial_f1, partial_precision,\n",
    "                    partial_recalls):\n",
    "    return {\"general_accuracy\": general_accuracy, \"exact_recalls\": exact_recalls,\n",
    "            \"partial_recalls\": partial_recalls, \"exact_precisions\": exact_precision,\n",
    "            \"partial_precisions\": partial_precision, \"exact_f1s\": exact_f1, \"partial_f1s\": partial_f1}\n",
    "\n",
    "\n",
    "def calculate_f1(recall, precision):\n",
    "    if precision + recall != 0:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def get_all_f1(exact_precision, exact_recalls, partial_precision, partial_recalls):\n",
    "    exact_f1 = {key: calculate_f1(exact_recalls[key], exact_precision[key]) for key in exact_recalls if\n",
    "                key in exact_precision}\n",
    "    partial_f1 = {key: calculate_f1(partial_recalls[key], partial_precision[key]) for key in partial_recalls if\n",
    "                  key in partial_precision}\n",
    "    return exact_f1, partial_f1\n",
    "\n",
    "\n",
    "def flatten_result_dict(evaluation_dict):\n",
    "    result = {}\n",
    "    for k, v in evaluation_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            for sub_k, sub_v in v.items():\n",
    "                result[f\"{k}_{sub_k}\"] = sub_v\n",
    "        else:\n",
    "            result[k] = v\n",
    "    return result\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def recall_json_based(label_list, pair_list, feature_name, mode=MatchType.EXACT.value):\n",
    "    \n",
    "    total_items_with_value_for_feature_in_labels = get_feature_based_denominator_count_json_based(feature_name,\n",
    "                                                                                                  label_list)\n",
    "   \n",
    "    total_items_with_correct_value_in_predictions = calculate_feature_based_tp_json_based(pair_list, feature_name, mode)\n",
    "    # print(feature_name,total_items_with_correct_value_in_predictions)\n",
    "\n",
    "    if total_items_with_value_for_feature_in_labels == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        recall = total_items_with_correct_value_in_predictions / total_items_with_value_for_feature_in_labels\n",
    "        assert 0 <= recall <= 1\n",
    "        # print(mode + \" \", feature_name, \"recall: \", recall, \"\\ttp: \",\n",
    "        #       total_items_with_correct_value_in_predictions, \"\\tp: \",\n",
    "        #       total_items_with_value_for_feature_in_labels)\n",
    "        return recall\n",
    "\n",
    "\n",
    "# tp/pp\n",
    "def precision_json_based(prediction_list, pair_list, feature_name, mode=MatchType.EXACT.value):\n",
    "    total_items_with_value_for_feature_in_predictions = get_feature_based_denominator_count_json_based(feature_name,\n",
    "                                                                                                       prediction_list)\n",
    "    total_items_with_correct_value_in_predictions = calculate_feature_based_tp_json_based(pair_list, feature_name, mode)\n",
    "\n",
    "    if total_items_with_value_for_feature_in_predictions == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        precision = total_items_with_correct_value_in_predictions / total_items_with_value_for_feature_in_predictions\n",
    "        assert 0 <= precision <= 1\n",
    "        # print(mode + \" \", feature_name, \"precision: \", precision, \"\\ttp: \",\n",
    "        #       total_items_with_correct_value_in_predictions, \"\\tpp: \",\n",
    "        #       total_items_with_value_for_feature_in_predictions)\n",
    "        return precision\n",
    "\n",
    "\n",
    "def parse_json_string(data):\n",
    "    if isinstance(data, (dict, list)):\n",
    "        return data\n",
    "    try:\n",
    "        parsed_json = json.loads(data)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError:\n",
    "        return data\n",
    "\n",
    "\n",
    "def compute_all_metrics_json_based(label_list, prediction_list):\n",
    "    label_prediction_pairs = make_list_of_pairs_json_based(label_list=label_list, prediction_list=prediction_list)\n",
    "    \n",
    "    general_accuracy = Metrics.general_accuracy_json_based(label_list=label_list, prediction_list=prediction_list)\n",
    "    exact_recalls, partial_recalls = Metrics.all_recall_json_based(label_list=label_list,\n",
    "                                                                   pair_list=label_prediction_pairs)\n",
    "\n",
    "    exact_precision, partial_precision = Metrics.all_precision_json_based(prediction_list=prediction_list,\n",
    "                                                                          pair_list=label_prediction_pairs)\n",
    "\n",
    "    exact_f1, partial_f1 = get_all_f1(exact_precision, exact_recalls, partial_precision, partial_recalls)\n",
    "\n",
    "    return get_result_dict(exact_f1, exact_precision, exact_recalls, general_accuracy, partial_f1,\n",
    "                           partial_precision, partial_recalls)\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    @staticmethod\n",
    "    # (tp+tn)/p+n\n",
    "    def general_accuracy_text_based(label_list, prediction_list):\n",
    "        tp = tn = 0\n",
    "        for label, prediction in zip(label_list, prediction_list):\n",
    "            if is_answerable(label) and is_answerable(prediction):\n",
    "                tp += 1\n",
    "            if is_unanswerable(label) and is_unanswerable(prediction):\n",
    "                tn += 1\n",
    "\n",
    "        accuracy = (tp + tn) / len(label_list)\n",
    "        assert 0 <= accuracy <= 1\n",
    "        return accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    # (tp+tn)/p+n\n",
    "    def general_accuracy_json_based(label_list, prediction_list):\n",
    "        tp = tn = 0\n",
    "        for label, prediction in zip(label_list, prediction_list):\n",
    "            label = parse_json_string(label)\n",
    "            prediction = parse_json_string(prediction)\n",
    "            if is_answerable(label) and is_answerable(prediction):\n",
    "                tp += 1\n",
    "            if is_unanswerable(label) and is_unanswerable(prediction):\n",
    "                tn += 1\n",
    "        accuracy = (tp + tn) / len(label_list)\n",
    "        assert 0 <= accuracy <= 1\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def all_recall_json_based(label_list, pair_list):\n",
    "\n",
    "        exact_results = {feature_name: recall_json_based(label_list, pair_list, feature_name, MatchType.EXACT.value) for\n",
    "                         feature_name in feature_names}\n",
    "        partial_results = {feature_name: recall_json_based(label_list, pair_list, feature_name, MatchType.PARTIAL.value)\n",
    "                           for\n",
    "                           feature_name in feature_names}\n",
    "\n",
    "        exact_results[\"overall\"] = sum(exact_results.values()) / len(exact_results)\n",
    "        partial_results[\"overall\"] = sum(partial_results.values()) / len(partial_results)\n",
    "\n",
    "        return exact_results, partial_results\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def all_precision_json_based(prediction_list, pair_list):\n",
    "\n",
    "        exact_results = {\n",
    "            feature_name: precision_json_based(prediction_list, pair_list, feature_name, MatchType.EXACT.value)\n",
    "            for feature_name in feature_names}\n",
    "        partial_results = {\n",
    "            feature_name: precision_json_based(prediction_list, pair_list, feature_name, MatchType.PARTIAL.value)\n",
    "            for feature_name in feature_names}\n",
    "\n",
    "        exact_results[\"overall\"] = sum(exact_results.values()) / len(exact_results)\n",
    "        partial_results[\"overall\"] = sum(partial_results.values()) / len(partial_results)\n",
    "\n",
    "        return exact_results, partial_results\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_property_wise_json_based(label_list, prediction_list):\n",
    "\n",
    "        evaluation_dict = compute_all_metrics_json_based(label_list, prediction_list)\n",
    "        return {k: round(v * 100, 4) for k, v in flatten_result_dict(evaluation_dict).items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_rouge(label_list, prediction_list):\n",
    "\n",
    "        metric = evaluate.load(\"rouge\")\n",
    "        result = metric.compute(predictions=prediction_list, references=label_list, use_stemmer=True)\n",
    "        return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "    # @staticmethod\n",
    "    # def evaluate_bleu(label_list, prediction_list):\n",
    "\n",
    "    #     metric = evaluate.load(\"bleu\")\n",
    "    #     result = metric.compute(predictions=prediction_list, references=label_list, use_stemmer=True)\n",
    "    #     return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "print(\"Evaluation method defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9269b69e",
   "metadata": {
    "_cell_guid": "1347cee8-d853-4763-9e7c-4e01813a3304",
    "_uuid": "12ba8383-c542-46fe-b512-c47022345191",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-09T13:04:46.282898Z",
     "iopub.status.busy": "2025-10-09T13:04:46.282396Z",
     "iopub.status.idle": "2025-10-09T13:04:46.288006Z",
     "shell.execute_reply": "2025-10-09T13:04:46.287136Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017255,
     "end_time": "2025-10-09T13:04:46.289010",
     "exception": true,
     "start_time": "2025-10-09T13:04:46.271755",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched '}' (1161675623.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_19/1161675623.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    labels = \"[{ \\\"geographical area\\\": \\\"West african\\\", \\\"local name\\\": \\\"Salt\\\", \\\"common name\\\": \\\"Salt\\\", \\\"food group\\\": \\\"Miscellaneous\\\", \\\"food components\\\": [{ \\\"label\\\": \\\"Salt01_Energy\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"Kcal\\\"} , { \\\"label\\\": \\\"Salt01_Water\\\", \\\"value\\\": 0.5, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Protein\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Carbohydrate\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Fibre\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Ash\\\", \\\"value\\\": 99.8, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Calcium\\\", \\\"value\\\": 216.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Fer\\\", \\\"value\\\": 1.2, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Magensium\\\", \\\"value\\\": 39.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Phosphore\\\", \\\"value\\\": 166.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Potassium\\\", \\\"value\\\": 1290.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Sodium\\\", \\\"value\\\": 38760.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Zinc\\\", \\\"\"}\"\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched '}'\n"
     ]
    }
   ],
   "source": [
    "# Test Mtrics\n",
    "\n",
    "def test_structure_evaluation():\n",
    "    labels = \"[{ \\\"geographical area\\\": \\\"West african\\\", \\\"local name\\\": \\\"Salt\\\", \\\"common name\\\": \\\"Salt\\\", \\\"food group\\\": \\\"Miscellaneous\\\", \\\"food components\\\": [{ \\\"label\\\": \\\"Salt01_Energy\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"Kcal\\\"} , { \\\"label\\\": \\\"Salt01_Water\\\", \\\"value\\\": 0.5, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Protein\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Carbohydrate\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Fibre\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Ash\\\", \\\"value\\\": 99.8, \\\"unit\\\": \\\"g\\\"} , { \\\"label\\\": \\\"Salt01_Calcium\\\", \\\"value\\\": 216.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Fer\\\", \\\"value\\\": 1.2, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Magensium\\\", \\\"value\\\": 39.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Phosphore\\\", \\\"value\\\": 166.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Potassium\\\", \\\"value\\\": 1290.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Sodium\\\", \\\"value\\\": 38760.0, \\\"unit\\\": \\\"mg\\\"} , { \\\"label\\\": \\\"Salt01_Zinc\\\", \\\"\"}\"\n",
    "\n",
    "    predictions = \"[ fructe \\\"geographical area\\\": \\\"North african\\\", \\\"local name\\\": \\\"Salt\\\", \\\"common name\\\": \\\"Salt\\\", \\\"food group\\\": \\\"Miscellaneous\\\", \\\"food components\\\": [ fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0, \\\"unit\\\": \\\"g\\\" fructe , fructe \\\"label\\\": \\\"Salt01_Fat\\\", \\\"value\\\": 0.0\"\n",
    "\n",
    "    results = Metrics.evaluate_property_wise_json_based(labels, predictions)\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "test_structure_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fab35",
   "metadata": {
    "_cell_guid": "ae54c3df-3b45-4c20-afc4-ff68adcf7409",
    "_uuid": "02fb3906-15f8-48b4-b9e1-8c41a8b5f018",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042fa8f",
   "metadata": {
    "_cell_guid": "42244057-f580-4bdf-bc93-d59b47d4aee4",
    "_uuid": "97e61986-8add-4ebb-9bbc-b019a2924170",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T04:25:42.671695Z",
     "iopub.status.busy": "2025-09-23T04:25:42.671065Z",
     "iopub.status.idle": "2025-09-23T04:26:13.043366Z",
     "shell.execute_reply": "2025-09-23T04:26:13.042603Z",
     "shell.execute_reply.started": "2025-09-23T04:25:42.671672Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORT =========================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from tokenizers import AddedToken\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "# from src.util.evaluation_metrics import Metrics\n",
    "\n",
    "print(\"\\nSuccessfull import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df00ce0",
   "metadata": {
    "_cell_guid": "288bb580-f035-4d69-85d1-8322f3f8a583",
    "_uuid": "3361a7c6-64df-42a7-b0c1-eb3b063fd3d9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The code below present code for fine-tuning Flan-T5 Model on knoledge Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c4d91",
   "metadata": {
    "_cell_guid": "9fb687df-3f0f-47cd-bdf4-44765ff3d55f",
    "_uuid": "4f79da0b-4e72-4747-a266-aaca791ced10",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b95db",
   "metadata": {
    "_cell_guid": "9fc264f7-bc0f-4156-8082-241da91f577a",
    "_uuid": "1f755896-b0e2-4bde-b476-6ee14d27f7ab",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T01:02:15.391164Z",
     "iopub.status.busy": "2025-09-17T01:02:15.390896Z",
     "iopub.status.idle": "2025-09-17T01:02:15.775407Z",
     "shell.execute_reply": "2025-09-17T01:02:15.774874Z",
     "shell.execute_reply.started": "2025-09-17T01:02:15.391142Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "seed = 42\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "num_epoch = 4\n",
    "run_number = 6\n",
    "run_id = f\"run{run_number}_extraction_epoch{num_epoch}\"\n",
    "path_prefix = \"/kaggle/working/\"\n",
    "model_is_from_train_set = 'drop_1'\n",
    "model_id = \"google/flan-t5-large\"\n",
    "max_target_length = 512\n",
    "max_source_length = 512\n",
    "label_pad_token_id = -100\n",
    "max_new_tokens = 512\n",
    "metric_name = \"partial_f1s_overall\"\n",
    "dataset_path = \"/kaggle/input/food-contribution-dataset/final_dataset_fixed.jsonl\"\n",
    "\n",
    "# \n",
    "dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "# découpage\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=seed)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "print(\"Train example:\\n\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "print(\"\\nTest example:\\n\")\n",
    "print(eval_dataset[0])\n",
    "\n",
    "# # Sauvegarder en JSON\n",
    "# with open(\"train_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(train_dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# with open(\"eval_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(eval_dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Fichiers JSON créés : train_dataset.json et eval_dataset.json\")\n",
    "\n",
    "print(\"\\nDataset loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708cf80",
   "metadata": {
    "_cell_guid": "e3239fef-ad9a-4b77-8d15-05cac22d73f4",
    "_uuid": "f6eac4af-16cf-4cec-8b77-983d71fbfd51",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Dataset Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7422e",
   "metadata": {
    "_cell_guid": "46a2a2e6-08c7-4931-8250-ea0ec5e0df03",
    "_uuid": "1f8693f5-2fc2-44ce-8ed6-13fd9af1ce38",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T01:02:24.021807Z",
     "iopub.status.busy": "2025-09-17T01:02:24.021181Z",
     "iopub.status.idle": "2025-09-17T01:02:45.236310Z",
     "shell.execute_reply": "2025-09-17T01:02:45.235445Z",
     "shell.execute_reply.started": "2025-09-17T01:02:24.021781Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "fine_tuned_model_repository = f\"{path_prefix}/models/json_based/{model_is_from_train_set}\"\n",
    "tokenizer_repository = f\"{path_prefix}/models/json_based/{model_is_from_train_set}/tokenizer\"\n",
    "if not os.path.exists(fine_tuned_model_repository):\n",
    "    os.makedirs(fine_tuned_model_repository)\n",
    "    os.makedirs(tokenizer_repository)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.add_tokens(AddedToken(\"{\", normalized=False))\n",
    "tokenizer.add_tokens(AddedToken(\"}\", normalized=False))\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "\n",
    "# def tokenize_function(sample):\n",
    "#     # tokenize inputs\n",
    "#     model_inputs = tokenizer(get_prompt(sample[\"input\"]), max_length=max_source_length, padding=\"max_length\", truncation=True,\n",
    "#                              return_tensors=\"pt\")\n",
    "\n",
    "#     # Tokenize targets with the `text_target` keyword argument\n",
    "#     # Convertir la cible JSON en texte\n",
    "#     target_text = json.dumps(sample[\"target\"], ensure_ascii=False)\n",
    "#     labels = tokenizer(text_target=target_text, max_length=max_target_length, padding=\"max_length\",\n",
    "#                        truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "#     labels[\"input_ids\"] = [\n",
    "#         [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]  # type: ignore\n",
    "#     ]\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    # Génère le prompt pour chaque entrée\n",
    "    inputs = [load_prompt(run_number, example) for example in batch[\"input\"]]\n",
    "\n",
    "    # Convertit chaque target (liste d'objets) en string JSON\n",
    "    targets = [json.dumps(t, ensure_ascii=False) for t in batch[\"target\"]]\n",
    "\n",
    "    # Tokenisation des inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_source_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenisation des targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    # Remplacer pad_token_id par -100 pour ignorer la perte\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "\n",
    "#     preds, labels = eval_preds\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "\n",
    "#     preds = np.where(preds != -100, preds, tokenizer.pad_token_id)  # type: ignore\n",
    "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # type: ignore\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#     result = Metrics.evaluate_property_wise_json_based(label_list=decoded_labels, prediction_list=decoded_preds) ## JE SUIS ICI\n",
    "#     result.update(Metrics.evaluate_rouge(label_list=decoded_labels, prediction_list=decoded_preds))\n",
    "#     return result\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)  # type: ignore\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # pour ameliorerer le decodage\n",
    "    # decoded_preds = [clean_json_text(p) for p in decoded_preds]\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # type: ignore\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # pour ameliorer le decodage\n",
    "    # decoded_labels = [clean_json_text(l) for l in decoded_labels]\n",
    "\n",
    "    # Enregistrement des sorties\n",
    "    with open(f\"{run_id}.jsonl\", \"w\") as f:\n",
    "        for pred, label in zip(decoded_preds, decoded_labels):\n",
    "            f.write(json.dumps({\"prediction\": pred, \"label\": label}) + \"\\n\")\n",
    "\n",
    "    # Calcul des métriques\n",
    "    result = Metrics.evaluate_property_wise_json_based(\n",
    "        label_list=decoded_labels,\n",
    "        prediction_list=decoded_preds\n",
    "    )\n",
    "    result.update(\n",
    "        Metrics.evaluate_rouge(\n",
    "            label_list=decoded_labels,\n",
    "            prediction_list=decoded_preds\n",
    "        )\n",
    "    )\n",
    "    # result.update(\n",
    "    #     Metrics.evaluate_bleu(\n",
    "    #         label_list=decoded_labels,\n",
    "    #         prediction_list=decoded_preds\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # save metric to json\n",
    "    # Sauvegarde en JSON (fichier)\n",
    "    with open(f\"{run_id}_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_tokenized_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"Keys of tokenized dataset: {list(train_tokenized_dataset.features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda799bb",
   "metadata": {
    "_cell_guid": "3f29cf6b-5653-4545-9321-f2d38a0eda8a",
    "_uuid": "978488f6-a5db-4bc7-bb1a-a0994bf9ebce",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e9b69",
   "metadata": {
    "_cell_guid": "94ec83de-1f13-49ee-9f59-058d77efb743",
    "_uuid": "b0fe1939-7073-4455-8fd1-bd852a177c7c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The code below present used metrics to evaluate our Flan-t5 model on knowledge Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a01a99",
   "metadata": {
    "_cell_guid": "5c135db0-3027-46dd-864a-33fc2b269ad0",
    "_uuid": "ed3c6704-8408-4df8-bdac-530389e985dd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-21T12:13:27.523365Z",
     "iopub.status.busy": "2025-08-21T12:13:27.522732Z",
     "iopub.status.idle": "2025-08-21T12:13:27.556759Z",
     "shell.execute_reply": "2025-08-21T12:13:27.556195Z",
     "shell.execute_reply.started": "2025-08-21T12:13:27.523338Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f876627",
   "metadata": {
    "_cell_guid": "1fc515fe-dcf2-4b42-9cec-78ce7bde1462",
    "_uuid": "7bea496f-2d08-4b5a-bff3-1ceb77f6d3ef",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21235f3",
   "metadata": {
    "_cell_guid": "179a87ee-f755-4879-a30a-abb57625abea",
    "_uuid": "138d348a-070f-4a29-bbd3-5c6dbc22282b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine tuning\n",
    "\n",
    "optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "lr_scheduler = AdafactorSchedule(optimizer)\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.001)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=fine_tuned_model_repository,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=num_epoch,\n",
    "    generation_max_length=max_new_tokens,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    greater_is_better=True,\n",
    "    logging_dir=f\"{fine_tuned_model_repository}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=eval_tokenized_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    callbacks=([early_stopping_callback])\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "tokenizer.save_pretrained(tokenizer_repository)\n",
    "best_ckpt_path = trainer.state.best_model_checkpoint\n",
    "print(f\"best epoch: {best_ckpt_path}\")\n",
    "\n",
    "save_training_curves_by_epoch(trainer, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d67449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T01:03:08.766356Z",
     "iopub.status.busy": "2025-09-17T01:03:08.766055Z",
     "iopub.status.idle": "2025-09-17T01:03:08.776314Z",
     "shell.execute_reply": "2025-09-17T01:03:08.775749Z",
     "shell.execute_reply.started": "2025-09-17T01:03:08.766333Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def save_training_curves_by_epoch(trainer, run_id, output_dir=\"training_curves\"):\n",
    "    \"\"\"\n",
    "    Enregistre les courbes d'entraînement et d'évaluation au format SVG par epoch.\n",
    "    \n",
    "    Args:\n",
    "        trainer : instance de Seq2SeqTrainer\n",
    "        run_id : identifiant unique pour préfixer les fichiers\n",
    "        output_dir : dossier où sauvegarder les SVG\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Récupérer l'historique du trainer\n",
    "    logs = trainer.state.log_history\n",
    "\n",
    "    # Extraire les courbes par epoch\n",
    "    epoch_losses = {}\n",
    "    for log in logs:\n",
    "        if 'epoch' in log:\n",
    "            epoch = log['epoch']\n",
    "            for k, v in log.items():\n",
    "                if k not in ['step', 'epoch', 'learning_rate']:\n",
    "                    if k not in epoch_losses:\n",
    "                        epoch_losses[k] = []\n",
    "                    epoch_losses[k].append((epoch, v))\n",
    "\n",
    "    # Plot Loss\n",
    "    if 'loss' in epoch_losses:\n",
    "        epochs, values = zip(*epoch_losses['loss'])\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(epochs, values, label='Train Loss')\n",
    "        if 'eval_loss' in epoch_losses:\n",
    "            epochs_eval, eval_values = zip(*epoch_losses['eval_loss'])\n",
    "            plt.plot(epochs_eval, eval_values, label='Eval Loss')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Evaluation Loss per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        loss_path = os.path.join(output_dir, f\"{run_id}_loss_epoch.svg\")\n",
    "        plt.savefig(loss_path, format='svg')\n",
    "        plt.close()\n",
    "        print(f\"Loss curve per epoch saved to {loss_path}\")\n",
    "\n",
    "    # Plot autres métriques\n",
    "    metric_keys = [k for k in epoch_losses.keys() if k not in ['loss','eval_loss']]\n",
    "    for key in metric_keys:\n",
    "        epochs, values = zip(*epoch_losses[key])\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(epochs, values, label=key)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(key)\n",
    "        plt.title(f\"Metric: {key} per Epoch\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        metric_path = os.path.join(output_dir, f\"{run_id}_{key}_epoch.svg\")\n",
    "        plt.savefig(metric_path, format='svg')\n",
    "        plt.close()\n",
    "        print(f\"Metric curve per epoch saved to {metric_path}\")\n",
    "\n",
    "print(\"\\nSave plot function defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d22762",
   "metadata": {
    "_cell_guid": "4ebbb605-336f-4bef-91c2-253953de1a21",
    "_uuid": "b17a73bf-456c-40a7-896f-888acbfcc7d4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c53869",
   "metadata": {
    "_cell_guid": "cf76983b-9e1c-4bd9-9949-6607d17a5a93",
    "_uuid": "9e324df2-5d42-41ad-8d44-fbb025cfd179",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Recharger le modèle et le tokenizer sauvegardés\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(best_ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_repository)\n",
    "\n",
    "trainer.model = model\n",
    "\n",
    "# 3. Générer des prédictions\n",
    "predictions = trainer.predict(eval_tokenized_dataset)\n",
    "preds = predictions.predictions\n",
    "\n",
    "# Décoder les prédictions\n",
    "decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "# Décoder les labels\n",
    "labels = np.where(predictions.label_ids != -100, predictions.label_ids, tokenizer.pad_token_id)\n",
    "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# 5. Sauvegarder en JSONL\n",
    "output_file = f\"predictions_run{run_number}.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        json.dump({\"prediction\": pred, \"label\": label}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Fichier JSONL sauvegardé: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27351f0c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# KE4KI pipeline ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505be720",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1. Load test papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5caa2b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T12:46:18.246659Z",
     "iopub.status.busy": "2025-10-09T12:46:18.246333Z",
     "iopub.status.idle": "2025-10-09T12:46:18.251065Z",
     "shell.execute_reply": "2025-10-09T12:46:18.250403Z",
     "shell.execute_reply.started": "2025-10-09T12:46:18.246639Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "west_african_food_composition_paper_dir = \"/kaggle/input/papers-ke4ki-test/ca7779b.PDF\"\n",
    "tunisia_food_composition_paper_dir = \"/kaggle/input/papers-ke4ki-test/tunisia food composition table 1 - 2007.pdf\"\n",
    "\n",
    "# choose paper for the test\n",
    "choosen_paper_dir = tunisia_food_composition_paper_dir\n",
    "\n",
    "print(\"Test papers loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e23f69",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Divide the choosen paper into pages, add prompt template and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c5c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T12:55:00.077931Z",
     "iopub.status.busy": "2025-10-09T12:55:00.077356Z",
     "iopub.status.idle": "2025-10-09T12:55:36.010396Z",
     "shell.execute_reply": "2025-10-09T12:55:36.009614Z",
     "shell.execute_reply.started": "2025-10-09T12:55:00.077894Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code ...\n",
    "import pdfplumber\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_pdf_pages(pdf_path):\n",
    "    segments = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                segments.append({\n",
    "                    \"page\": i + 1,\n",
    "                    \"text\": text.strip()\n",
    "                })\n",
    "    return segments\n",
    "\n",
    "pages = extract_pdf_pages(choosen_paper_dir)\n",
    "\n",
    "print(\"\\nThe choosen paper has been divided into pages\\n\")\n",
    "\n",
    "print(pages[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ee767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T12:55:41.757951Z",
     "iopub.status.busy": "2025-10-09T12:55:41.757676Z",
     "iopub.status.idle": "2025-10-09T12:55:41.763036Z",
     "shell.execute_reply": "2025-10-09T12:55:41.762354Z",
     "shell.execute_reply.started": "2025-10-09T12:55:41.757931Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORT =========================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from tokenizers import AddedToken\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EarlyStoppingCallback\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "# from src.util.evaluation_metrics import Metrics\n",
    "\n",
    "print(\"\\nSuccessfull import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac91c59",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015e02e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T12:55:49.395032Z",
     "iopub.status.busy": "2025-10-09T12:55:49.394716Z",
     "iopub.status.idle": "2025-10-09T12:55:51.874336Z",
     "shell.execute_reply": "2025-10-09T12:55:51.873678Z",
     "shell.execute_reply.started": "2025-10-09T12:55:49.395012Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code ...\n",
    "tokenizer_dir = \"/kaggle/working/models/json_based/drop_1/tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "tokenizer.add_tokens(AddedToken(\"{\", normalized=False))\n",
    "tokenizer.add_tokens(AddedToken(\"}\", normalized=False))\n",
    "model_checkpoint = \"/kaggle/working/models/json_based/drop_1/checkpoint-258\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Si GPU dispo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"\\nModel loaded successfully\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e13a2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Perform extraction and validate model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6e615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T12:57:49.036180Z",
     "iopub.status.busy": "2025-10-09T12:57:49.035545Z",
     "iopub.status.idle": "2025-10-09T12:58:10.126380Z",
     "shell.execute_reply": "2025-10-09T12:58:10.125735Z",
     "shell.execute_reply.started": "2025-10-09T12:57:49.036156Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code ...\n",
    "from json_repair import repair_json\n",
    "\n",
    "def predict_one(text, max_new_tokens=512, num_beams=4):\n",
    "    \"\"\"\n",
    "    Prend un seul texte en entrée et retourne la prédiction.\n",
    "    \"\"\"\n",
    "    # Tokenisation\n",
    "    inputs = tokenizer(\n",
    "        load_prompt(6, text),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Génération\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams\n",
    "        )\n",
    "\n",
    "    # Conversion en numpy et nettoyage comme dans compute_metrics\n",
    "    preds = outputs.cpu().numpy()\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "\n",
    "    # Décodage cohérent avec compute_metrics\n",
    "    decoded_pred = tokenizer.decode(preds[0], skip_special_tokens=True)\n",
    "\n",
    "    return decoded_pred\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Limit the list to use only one page (Page 20 for example)\n",
    "page_test = []\n",
    "page_test.append(pages[20])\n",
    "for p in page_test:\n",
    "    prediction = predict_one(p.get(\"text\"))\n",
    "    predictions.append({\n",
    "        \"paper\": choosen_paper_dir,\n",
    "        \"page\":p.get(\"page\"),\n",
    "        \"extracted_data\": repair_json(prediction)\n",
    "    })\n",
    "\n",
    "print(\"\\nExtraction finished successfully\\n\")\n",
    "print(predictions[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f104e92",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Index extracted items into orkg and save links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccefadf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T12:58:33.656848Z",
     "iopub.status.busy": "2025-10-09T12:58:33.656607Z",
     "iopub.status.idle": "2025-10-09T12:58:33.661109Z",
     "shell.execute_reply": "2025-10-09T12:58:33.660446Z",
     "shell.execute_reply.started": "2025-10-09T12:58:33.656833Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code ...\n",
    "\n",
    "from orkg import ORKG, Hosts\n",
    "from difflib import get_close_matches\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"\\nNecessary libs imported !\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a35695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T12:49:14.323863Z",
     "iopub.status.busy": "2025-10-09T12:49:14.323278Z",
     "iopub.status.idle": "2025-10-09T12:49:46.979477Z",
     "shell.execute_reply": "2025-10-09T12:49:46.978887Z",
     "shell.execute_reply.started": "2025-10-09T12:49:14.323837Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Connexion au sandbox ORKG\n",
    "# -----------------------------------------\n",
    "sandbox_url = \"https://sandbox.orkg.org\"\n",
    "orkg = ORKG(host=Hosts.SANDBOX, creds=('emmanuelleuna758@gmail.com', 'Leuna75820u2698@'))\n",
    "orkg_resources_manager = orkg.resources\n",
    "orkg_statements_manager = orkg.statements\n",
    "orkg_litterals_manager = orkg.literals\n",
    "\n",
    "# -----------------------------------------\n",
    "# Get template properties\n",
    "# -----------------------------------------\n",
    "TEMPLATE_ID = \"R677398\"\n",
    "TEMPLATE_PROPERTIES = [\n",
    "    # Food template --------------------------\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677462\",\n",
    "        \"propertyShapePath\": \"P142021\",\n",
    "        \"propertyShapePathLabel\": \"food type\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677459\",\n",
    "        \"propertyShapePath\": \"P142023\",\n",
    "        \"propertyShapePathLabel\": \"scientific name\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677464\",\n",
    "        \"propertyShapePath\": \"P20098\",\n",
    "        \"propertyShapePathLabel\": \"has description\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677465\",\n",
    "        \"propertyShapePath\": \"P135009\",\n",
    "        \"propertyShapePathLabel\": \"geographical area\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677457\",\n",
    "        \"propertyShapePath\": \"P142020\",\n",
    "        \"propertyShapePathLabel\": \"local name\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677461\",\n",
    "        \"propertyShapePath\": \"P142022\",\n",
    "        \"propertyShapePathLabel\": \"food form\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677458\",\n",
    "        \"propertyShapePath\": \"P142024\",\n",
    "        \"propertyShapePathLabel\": \"common name\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R1361187\",\n",
    "        \"propertyShapePath\": \"P142026\",\n",
    "        \"propertyShapePathLabel\": \"food image\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R707241\",\n",
    "        \"propertyShapePath\": \"P151019\",\n",
    "        \"propertyShapePathLabel\": \"eaten with\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R707751\",\n",
    "        \"propertyShapePath\": \"P35126\",\n",
    "        \"propertyShapePathLabel\": \"Group\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677460\",\n",
    "        \"propertyShapePath\": \"P6003\",\n",
    "        \"propertyShapePathLabel\": \"has ingredient\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R707240\",\n",
    "        \"propertyShapePath\": \"P62002\",\n",
    "        \"propertyShapePathLabel\": \"food group\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food template\",\n",
    "        \"propertyShape\": \"R677466\",\n",
    "        \"propertyShapePath\": \"P62073\",\n",
    "        \"propertyShapePathLabel\": \"food component\"\n",
    "    },\n",
    "    \n",
    "    # Food component template --------------------------\n",
    "    {\n",
    "        \"label\": \"Food component template\",\n",
    "        \"propertyShape\": \"R684340\",\n",
    "        \"propertyShapePath\": \":P62093\",\n",
    "        \"propertyShapePathLabel\": \"food component name\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Food component template\",\n",
    "        \"propertyShape\": \"R684342\",\n",
    "        \"propertyShapePath\": \":P5086\",\n",
    "        \"propertyShapePathLabel\": \"value\"\n",
    "    },\n",
    "    \n",
    "    # Quantity Value template --------------------------\n",
    "    {\n",
    "        \"label\": \"Quantity Value\",\n",
    "        \"propertyShape\": \"R172467\",\n",
    "        \"propertyShapePath\": \":P45075\",\n",
    "        \"propertyShapePathLabel\": \"numericValue\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Quantity Value\",\n",
    "        \"propertyShape\": \"R172468\",\n",
    "        \"propertyShapePath\": \":P45076\",\n",
    "        \"propertyShapePathLabel\": \"unit\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# -----------------------------------------\n",
    "# Functions\n",
    "# -----------------------------------------\n",
    "\n",
    "# Find or add resource\n",
    "def find_or_add_resource(label, instanceOf):\n",
    "    \n",
    "    r = orkg.resources.get_unpaginated(q=label,exact=True, include=instanceOf)\n",
    "    r = r.content\n",
    "   \n",
    "    if(len(r)>0):\n",
    "        return r[0]\n",
    "    else:\n",
    "        # print(instanceOf)\n",
    "        r = orkg_resources_manager.add(label=label, classes=instanceOf)\n",
    "        print('Resource ' + label+ \" added\")\n",
    "        return r.content\n",
    "\n",
    "# Get predicate id from our template\n",
    "def get_predicate_id(predicate_string):\n",
    "    predicate_id = None\n",
    "    for item in TEMPLATE_PROPERTIES:\n",
    "        if item['propertyShapePathLabel'] == predicate_string:\n",
    "            predicate_id = item['propertyShapePath']\n",
    "            return predicate_id\n",
    "\n",
    "# add statementitem\n",
    "def add_statement(predicate_string, subject_id, object_classes, extracted_item, object_id=None):\n",
    "    predicate_found = 0\n",
    "    if(object_id == None):\n",
    "        extracted_item = dict(extracted_item)\n",
    "        label_column = get_close_matches(predicate_string, extracted_item.keys(), n=1, cutoff=cutoff)\n",
    "        if(len(label_column)>0):\n",
    "            predicate_found = 1\n",
    "            label = label_column[0]\n",
    "            value = extracted_item[label]\n",
    "            object_resource = find_or_add_resource(label=value, instanceOf=object_classes)\n",
    "            object_id = object_resource['id']\n",
    "            \n",
    "    if(predicate_found == 1):\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add {predicate_string} statement\n",
    "        # -----------------------------------------\n",
    "        # get statements by subject and predicate without paggination\n",
    "        statements_subject_predicate = orkg_statements_manager.get_by_subject_and_predicate_unpaginated(subject_id, get_predicate_id(predicate_string))\n",
    "        statements_subject_predicate = statements_subject_predicate.content\n",
    "        # get statements by predicate and object without paggination\n",
    "        statements_object_predicate = orkg_statements_manager.get_by_object_and_predicate_unpaginated(object_id, get_predicate_id(predicate_string))\n",
    "        statements_object_predicate = statements_object_predicate.content\n",
    "        # check if statement already exist\n",
    "        if(statements_subject_predicate.__len__() == 0 | statements_object_predicate.__len__() == 0):\n",
    "            print('The statement '+subject_id+' '+predicate_string+' '+object_id +' does not exist')\n",
    "            print('Adding of the statement ...')\n",
    "            # add statement\n",
    "            orkg_statements_manager.add(subject_id=subject_id, predicate_id=get_predicate_id(predicate_string), object_id=object_id)\n",
    "        else:\n",
    "            # Obtenir les IDs de chaque liste\n",
    "            ids_liste2 = {item[\"id\"] for item in statements_object_predicate}\n",
    "\n",
    "            # Intersection : éléments de liste1 dont l'id est dans liste2\n",
    "            intersection = [item for item in statements_subject_predicate if item[\"id\"] in ids_liste2]\n",
    "            if(intersection.__len__() == 0):\n",
    "                print('The statement '+subject_id+' '+predicate_string+' '+object_id +' does not exist')\n",
    "                print('Adding of the statement ...')\n",
    "                # add statement\n",
    "                orkg_statements_manager.add(subject_id=subject_id, predicate_id=get_predicate_id(predicate_string), object_id=object_id)\n",
    "            else: \n",
    "                print('The statement '+subject_id+' '+predicate_string+' '+object_id +' already exist')\n",
    "                print('skipping of the statement ...')\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Get extracted data\n",
    "# -----------------------------------------\n",
    "all_pred = predictions\n",
    "    \n",
    "\n",
    "# Fusionner toutes les prédictions en une seule liste\n",
    "merged_predictions = []\n",
    "\n",
    "for item in all_pred:\n",
    "    predictions_temp = item.get(\"extracted_data\", [])\n",
    "    print(predictions_temp)\n",
    "    predictions_temp = json.loads(predictions_temp)\n",
    "    # Ajoute chaque prédiction à la liste globale\n",
    "    for pred in predictions_temp:\n",
    "        merged_predictions.append(pred)\n",
    "\n",
    "data = merged_predictions\n",
    "\n",
    "# data = [\n",
    "#   {\n",
    "#     \"geographical area\": \"West african\",\n",
    "#     \"local name\": \"Salt\",\n",
    "#     \"common name\": \"Salt\",\n",
    "#     \"food group\": \"Miscellaneous\",\n",
    "#     \"food components\": [\n",
    "#       { \"label\": \"Salt01_Energy\", \"value\": 0.0, \"unit\": \"Kcal\" },\n",
    "#       { \"label\": \"Salt01_Water\", \"value\": 0.5, \"unit\": \"g\" },\n",
    "#       { \"label\": \"Salt01_Protein\", \"value\": 0.0, \"unit\": \"g\" },\n",
    "#       { \"label\": \"Salt01_Fat\", \"value\": 0.0, \"unit\": \"g\" },\n",
    "#       { \"label\": \"Salt01_Carbohydrate\", \"value\": 0.0, \"unit\": \"g\" },\n",
    "#       { \"label\": \"Salt01_Fibre\", \"value\": 0.0, \"unit\": \"g\" },\n",
    "#       { \"label\": \"Salt01_Ash\", \"value\": 99.8, \"unit\": \"g\" },\n",
    "#       { \"label\": \"Salt01_Calcium\", \"value\": 216.0, \"unit\": \"mg\" },\n",
    "#       { \"label\": \"Salt01_Fer\", \"value\": 1.2, \"unit\": \"mg\" },\n",
    "#       { \"label\": \"Salt01_Magnesium\", \"value\": 39.0, \"unit\": \"mg\" },\n",
    "#       { \"label\": \"Salt01_Phosphore\", \"value\": 166.0, \"unit\": \"mg\" },\n",
    "#       { \"label\": \"Salt01_Potassium\", \"value\": 1290.0, \"unit\": \"mg\" },\n",
    "#       { \"label\": \"Salt01_Sodium\", \"value\": 38760.0, \"unit\": \"mg\" },\n",
    "#       { \"label\": \"Salt01_Zinc\", \"value\": 0.1, \"unit\": \"mg\" }\n",
    "#     ]\n",
    "#   }\n",
    "# ]\n",
    "\n",
    "# Start the timer\n",
    "start = time.time()\n",
    "\n",
    "# -----------------------------------------\n",
    "# Map extracted data\n",
    "# -----------------------------------------\n",
    "mapped_data = []\n",
    "cutoff = 0.90\n",
    "for item in data:\n",
    "    properties_found = []\n",
    "    for property in TEMPLATE_PROPERTIES:\n",
    "        label_found = get_close_matches(property[\"propertyShapePathLabel\"], item.keys(), n=1, cutoff=cutoff)\n",
    "        \n",
    "        if(len(label_found) >0):\n",
    "            properties_found.append(label_found[0])\n",
    "            \n",
    "    # get value in item for each property foud\n",
    "    item_properties = {}\n",
    "    for prop in properties_found:\n",
    "        item_properties[prop] = item[prop]\n",
    "    \n",
    "    # add mapped item to mapped_data\n",
    "    mapped_data.append(item_properties)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "indexed_statement = []\n",
    "resources_urls = []\n",
    "for item in mapped_data:\n",
    "    # -----------------------------------------\n",
    "    # Create or get the general ressource (Food)\n",
    "    # -----------------------------------------\n",
    "    nb_statement = 0\n",
    "    item = dict(item)\n",
    "    label_column = get_close_matches(\"local name\", item.keys(), n=1, cutoff=cutoff)\n",
    "    if(label_column.__len__() > 0):\n",
    "        label = label_column[0]\n",
    "        value = item[label]\n",
    "        general_resource = find_or_add_resource(label=value, instanceOf=['C34019'])\n",
    "        resource_url = f\"https://sandbox.orkg.org/resources/{general_resource['id']}?noRedirect\"\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add local name statement\n",
    "        # -----------------------------------------\n",
    "        # Here the object is also the subject\n",
    "        add_statement(predicate_string=\"local name\", subject_id=general_resource['id'], object_classes=['C34019'], extracted_item=item)\n",
    "        nb_statement += 1\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add scientific name statement\n",
    "        # -----------------------------------------\n",
    "        if( get_close_matches(\"local name\", item.keys(), n=1, cutoff=cutoff).__len__() >0):\n",
    "            add_statement(\"scientific name\", general_resource['id'], ['C86010'], item)\n",
    "            nb_statement += 1\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add common name statement\n",
    "        # -----------------------------------------\n",
    "        if(get_close_matches(\"common name\", item.keys(), n=1, cutoff=cutoff).__len__() >0):\n",
    "            add_statement(\"common name\", general_resource['id'], ['C34019'], item)\n",
    "            nb_statement += 1\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add food group statement\n",
    "        # -----------------------------------------\n",
    "        if(get_close_matches(\"food group\", item.keys(), n=1, cutoff=cutoff).__len__() >0):\n",
    "            add_statement(\"food group\", general_resource['id'], ['C34000'], item)\n",
    "            nb_statement += 1\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add geographical area statement\n",
    "        # -----------------------------------------\n",
    "        if(get_close_matches(\"geographical area\", item.keys(), n=1, cutoff=cutoff).__len__() >0):\n",
    "            add_statement(\"geographical area\", general_resource['id'], ['C86012'], item)\n",
    "            nb_statement += 1\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add food form statement\n",
    "        # -----------------------------------------\n",
    "        if(get_close_matches(\"food form\", item.keys(), n=1, cutoff=cutoff).__len__() >0):\n",
    "            add_statement(\"food form\", general_resource['id'], ['C86013'], item)\n",
    "            nb_statement += 1\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add food components statement\n",
    "        # -----------------------------------------\n",
    "        if(get_close_matches(\"food components\", item.keys(), n=1, cutoff=cutoff).__len__() >0):\n",
    "            for component in item['food components']:\n",
    "                component = dict(component)\n",
    "                c = find_or_add_resource(label=item['local name']+'_'+ component['label'],instanceOf=[\"C34009\"])                \n",
    "                # add food component name statement\n",
    "                \n",
    "                if( \"label\" in component.keys()):\n",
    "                    add_statement(\"food component name\", c['id'], ['C34009'], {\n",
    "                        \"food component name\": component[\"label\"]\n",
    "                    })\n",
    "                    \n",
    "                # add food component value statement C23008\n",
    "                if( \"value\" in component.keys() and \"unit\" in component.keys()):\n",
    "                    v = find_or_add_resource(label=str(component['value'])+' '+ str(component['unit']),instanceOf=[\"C23008\"])\n",
    "\n",
    "                    # add numericValue value statement\n",
    "                    l = orkg_litterals_manager.get_all(q=component[\"value\"], exact=True)\n",
    "                    l = l.content\n",
    "                    if(len(l)==0 ):\n",
    "                        l = orkg_litterals_manager.add(label=component[\"value\"], datatype='xsd:decimal')\n",
    "                        l = l.content\n",
    "                    else:\n",
    "                        l = l[0]\n",
    "                    add_statement(\"numericValue\", v['id'], ['literal'], None, l['id'])\n",
    "\n",
    "                    # add unit value statement\n",
    "                    add_statement(\"unit\", v['id'], ['C23009'], component)\n",
    "                    \n",
    "                    add_statement(\"value\", c['id'], ['C23008'], None, v['id'])\n",
    "                    \n",
    "                    \n",
    "                add_statement(\"food components\", general_resource[\"id\"], ['C34009'], None, c[\"id\"])\n",
    "                nb_statement += 1\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # add food ingredient statement\n",
    "        # -----------------------------------------\n",
    "        if( \"ingredients\" in item.keys()):\n",
    "            for ingredient in item['ingredients']:\n",
    "                ingredient = dict(ingredient)\n",
    "                i = find_or_add_resource(label=item['local name']+'_ingredient'+ ingredient['label'], instanceOf=\"C77007\")\n",
    "                \n",
    "                # add food ingredient common name statement\n",
    "                if( \"common name\" in ingredient.keys()):\n",
    "                    add_statement(\"common name\", i['id'], ['C34019'], ingredient)\n",
    "                    \n",
    "                # add food ingredient scientific name statement\n",
    "                if( \"scientific name\" in ingredient.keys()):\n",
    "                    add_statement(\"scientific name\", i['id'], ['C86010'], ingredient)\n",
    "                    \n",
    "                add_statement(\"has ingredient\", general_resource[\"id\"], ['C77007'], None, i[\"id\"])\n",
    "                nb_statement += 1\n",
    "                \n",
    "        # get the resource statements\n",
    "        general_ressources_statements = orkg_statements_manager.get_by_subject_unpaginated(general_resource['id'])\n",
    "        general_ressources_statements = general_ressources_statements.content\n",
    "        indexed_statement.append(general_ressources_statements)\n",
    "        \n",
    "        # print statement list\n",
    "        print(f\"\\n\\nnumber of statements:  {general_ressources_statements.__len__()}\")\n",
    "        \n",
    "        # adding resource url\n",
    "        resources_urls.append({\n",
    "            \"label\":value,\n",
    "            \"url\": resource_url\n",
    "        })\n",
    "\n",
    "        print(f'URL: {resource_url} ')\n",
    "    else:\n",
    "        print('This ressource can not be index')\n",
    "\n",
    "\n",
    "json.dump(resources_urls, open('resources_urls_wfct.json', 'w'), indent=4)\n",
    "# -----------------------------------------\n",
    "# \n",
    "# -----------------------------------------\n",
    "\n",
    "# Stop the timer\n",
    "end = time.time()\n",
    "print(f\"Execution time : {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ebf0b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13498684,
     "datasetId": 7624380,
     "sourceId": 12858852,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13805875,
     "datasetId": 7646904,
     "sourceId": 13120788,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 14015686,
     "datasetId": 8326084,
     "sourceId": 13312374,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.052465,
   "end_time": "2025-10-09T13:04:49.208004",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-09T13:03:35.155539",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
